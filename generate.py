"""
Text Generation Module for Character-Level LSTM Model
======================================================
Bu modÃ¼l, eÄŸitilmiÅŸ LSTM modelini kullanarak Tolstoy stilinde metin Ã¼retir.

Temel iÅŸlevler:
- Model ve vocabulary yÃ¼kleme
- Temperature-based sampling ile metin Ã¼retimi
- CLI arayÃ¼zÃ¼ ile kolay kullanÄ±m
- FarklÄ± seed text'ler ve parametrelerle deneme yapma
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, Tuple

# TensorFlow uyarÄ±larÄ±nÄ± kapat
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0=all, 1=info, 2=warning, 3=error
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # oneDNN uyarÄ±larÄ±nÄ± kapat

import numpy as np
from tensorflow.keras.models import load_model, Model

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TextGenerator:
    """LSTM modeli ile metin Ã¼retme sÄ±nÄ±fÄ±"""
    
    def __init__(self, artifacts_dir: str = "artifacts", seq_length: int = 40):
        """
        Args:
            artifacts_dir: Model ve vocabulary dosyalarÄ±nÄ±n bulunduÄŸu dizin
            seq_length: Girdi sequence uzunluÄŸu (model eÄŸitimindekiyle aynÄ± olmalÄ±)
        """
        self.script_dir = Path(__file__).parent
        self.artifacts_dir = (self.script_dir / artifacts_dir).resolve()
        self.seq_length = seq_length
        
        self.model: Model = None
        self.char_to_idx: Dict[str, int] = {}
        self.idx_to_char: Dict[str, str] = {}
        self.vocab_size: int = 0
        
        logger.info(f"Artifacts directory: {self.artifacts_dir}")
        
    def load_model_and_vocab(self, verbose: bool = True) -> None:
        """Model ve vocabulary dosyalarÄ±nÄ± yÃ¼kler"""
        try:
            # Model yÃ¼kleme
            model_path = self.artifacts_dir / "best_model.keras"
            if not model_path.exists():
                raise FileNotFoundError(f"Model dosyasÄ± bulunamadÄ±: {model_path}")
            
            if verbose:
                print("ğŸ”„ Model yÃ¼kleniyor...")
            self.model = load_model(model_path)
            if verbose:
                print("âœ… Model baÅŸarÄ±yla yÃ¼klendi")
            
            # Vocabulary yÃ¼kleme
            char_to_idx_path = self.artifacts_dir / "char_to_idx.json"
            idx_to_char_path = self.artifacts_dir / "idx_to_char.json"
            
            if not char_to_idx_path.exists() or not idx_to_char_path.exists():
                raise FileNotFoundError("Vocabulary dosyalarÄ± bulunamadÄ±")
            
            with open(char_to_idx_path, "r", encoding="utf-8") as f:
                self.char_to_idx = json.load(f)
            
            with open(idx_to_char_path, "r", encoding="utf-8") as f:
                self.idx_to_char = json.load(f)
            
            self.vocab_size = len(self.char_to_idx)
            if verbose:
                print(f"âœ… Vocabulary yÃ¼klendi (boyut: {self.vocab_size})")
            
        except Exception as e:
            logger.error(f"Model veya vocabulary yÃ¼klenirken hata oluÅŸtu: {e}")
            raise
    
    def sample_with_temperature(self, predictions: np.ndarray, temperature: float = 1.0) -> int:
        """
        Temperature-based sampling ile sonraki karakteri seÃ§er
        
        Args:
            predictions: Model Ã§Ä±ktÄ±sÄ± (probability distribution)
            temperature: Sampling sÄ±caklÄ±ÄŸÄ±
                - DÃ¼ÅŸÃ¼k (0.2-0.5): Daha deterministik, gÃ¼venli Ã§Ä±ktÄ±
                - Orta (0.5-1.0): Dengeli
                - YÃ¼ksek (1.0+): Daha yaratÄ±cÄ±, riskli
        
        Returns:
            SeÃ§ilen karakterin index'i
        """
        predictions = np.asarray(predictions).astype("float64")
        predictions = np.log(predictions + 1e-8) / temperature
        exp_predictions = np.exp(predictions)
        predictions = exp_predictions / np.sum(exp_predictions)
        
        return np.random.choice(len(predictions), p=predictions)
    
    def generate_text(
        self,
        seed_text: str,
        length: int = 400,
        temperature: float = 0.5,
        verbose: bool = False
    ) -> str:
        """
        Verilen seed text'ten baÅŸlayarak metin Ã¼retir
        
        Args:
            seed_text: BaÅŸlangÄ±Ã§ metni
            length: Ãœretilecek karakter sayÄ±sÄ±
            temperature: Sampling sÄ±caklÄ±ÄŸÄ±
            verbose: Ä°lerleme mesajlarÄ±nÄ± gÃ¶ster
        
        Returns:
            Ãœretilen metin (seed text dahil)
        """
        if self.model is None:
            raise RuntimeError("Model yÃ¼klenmemiÅŸ. Ã–nce load_model_and_vocab() Ã§aÄŸÄ±rÄ±n.")
        
        generated = seed_text.lower()
        
        if verbose:
            print(f"â³ Metin Ã¼retiliyor (temperature: {temperature})...")
        
        for i in range(length):
            # Son SEQ_LENGTH karakteri al
            seq = generated[-self.seq_length:]
            
            # Padding (seed kÄ±sa ise)
            if len(seq) < self.seq_length:
                seq = " " * (self.seq_length - len(seq)) + seq
            
            # Karakterleri index'lere Ã§evir
            x = np.zeros((1, self.seq_length), dtype=np.int32)
            for t, char in enumerate(seq):
                x[0, t] = self.char_to_idx.get(char, 0)
            
            # Tahmin yap
            predictions = self.model.predict(x, verbose=0)[0]
            next_idx = self.sample_with_temperature(predictions, temperature)
            next_char = self.idx_to_char[str(next_idx)]
            
            generated += next_char
            
            if verbose:
                logger.debug(f"{i + 1}/{length} karakter Ã¼retildi")
        
        logger.info("Metin Ã¼retimi tamamlandÄ±")
        return generated
    
    def generate_multiple(
        self,
        seed_text: str,
        length: int = 400,
        temperatures: Tuple[float, ...] = (0.2, 0.5, 1.0),
        verbose: bool = False
    ) -> Dict[float, str]:
        """
        FarklÄ± temperature deÄŸerleriyle birden fazla metin Ã¼retir
        
        Args:
            seed_text: BaÅŸlangÄ±Ã§ metni
            length: Ãœretilecek karakter sayÄ±sÄ±
            temperatures: Denenecek temperature deÄŸerleri
            verbose: Ä°lerleme mesajlarÄ±nÄ± gÃ¶ster
        
        Returns:
            Temperature -> Ã¼retilen metin dictionary'si
        """
        results = {}
        
        print(f"\nğŸ“ Seed Text: '{seed_text}'")
        print(f"ğŸ“Š Karakter sayÄ±sÄ±: {length}")
        print(f"ğŸŒ¡ï¸  Temperature deÄŸerleri: {temperatures}\n")
        
        for i, temp in enumerate(temperatures, 1):
            print(f"\n{'=' * 70}")
            print(f"  [{i}/{len(temperatures)}] Temperature: {temp}")
            print('=' * 70)
            
            generated_text = self.generate_text(seed_text, length, temp, verbose)
            results[temp] = generated_text
            print(generated_text)
            print()
        return results


def main():
    """Ana fonksiyon - CLI arayÃ¼zÃ¼"""
    parser = argparse.ArgumentParser(
        description="LSTM modeli ile Tolstoy stilinde metin Ã¼retimi"
    )
    parser.add_argument(
        "--seed",
        type=str,
        default="the old man looked at",
        help="BaÅŸlangÄ±Ã§ metni (seed text)"
    )
    parser.add_argument(
        "--length",
        type=int,
        default=400,
        help="Ãœretilecek karakter sayÄ±sÄ±"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        nargs="+",
        default=[0.2, 0.5, 1.0],
        help="Temperature deÄŸerleri (Ã¶rn: --temperature 0.5 veya --temperature 0.2 0.5 1.0)"
    )
    parser.add_argument(
        "--artifacts-dir",
        type=str,
        default="artifacts",
        help="Model ve vocabulary dosyalarÄ±nÄ±n bulunduÄŸu dizin"
    )
    parser.add_argument(
        "--seq-length",
        type=int,
        default=40,
        help="Girdi sequence uzunluÄŸu (model eÄŸitimindekiyle aynÄ± olmalÄ±)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="DetaylÄ± ilerleme mesajlarÄ± gÃ¶ster"
    )
    
    args = parser.parse_args()
    
    try:
        print("\n" + "=" * 70)
        print("  ğŸ­ Tolstoy Style Text Generator")
        print("=" * 70)
        
        # Generator oluÅŸtur ve model yÃ¼kle
        generator = TextGenerator(
            artifacts_dir=args.artifacts_dir,
            seq_length=args.seq_length
        )
        generator.load_model_and_vocab(verbose=True)
        
        # Metin Ã¼ret
        generator.generate_multiple(
            seed_text=args.seed,
            length=args.length,
            temperatures=tuple(args.temperature),
            verbose=args.verbose
        )
        
        print("=" * 70)
        print("âœ… Metin Ã¼retimi tamamlandÄ±!")
        print("=" * 70)
        
    except Exception as e:
        logger.error(f"Hata oluÅŸtu: {e}")
        raise


if __name__ == "__main__":
    main()
