"""
Streamlit Web Application for Tolstoy Style Text Generation
============================================================
Bu Streamlit uygulamasÄ±, eÄŸitilmiÅŸ LSTM modelini kullanarak
Tolstoy stilinde metin Ã¼retimi yapar.

Ã–zellikler:
- Interaktif web arayÃ¼zÃ¼
- Temperature kontrolÃ¼ ile yaratÄ±cÄ±lÄ±k ayarÄ±
- Seed text ile Ã¶zelleÅŸtirilebilir baÅŸlangÄ±Ã§
- GerÃ§ek zamanlÄ± metin Ã¼retimi
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, Tuple

# TensorFlow uyarÄ±larÄ±nÄ± kapat
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import numpy as np
import streamlit as st
from tensorflow.keras.models import load_model, Model

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =========================
# CONFIG
# =========================
class Config:
    """Uygulama yapÄ±landÄ±rma ayarlarÄ±"""
    SCRIPT_DIR = Path(__file__).parent
    ARTIFACTS_DIR = (SCRIPT_DIR / "artifacts").resolve()
    MODEL_PATH = ARTIFACTS_DIR / "best_model.keras"
    SEQ_LENGTH = 40
    
    # UI VarsayÄ±lan deÄŸerleri
    DEFAULT_SEED = "the old man looked at"
    DEFAULT_TEMPERATURE = 0.5
    DEFAULT_LENGTH = 400
    MIN_TEMPERATURE = 0.1
    MAX_TEMPERATURE = 1.2
    MIN_LENGTH = 100
    MAX_LENGTH = 800


# =========================
# TEXT GENERATOR CLASS
# =========================
class StreamlitTextGenerator:
    """Streamlit iÃ§in optimize edilmiÅŸ metin Ã¼retici sÄ±nÄ±fÄ±"""
    
    def __init__(self, artifacts_dir: Path, seq_length: int = 40):
        """
        Args:
            artifacts_dir: Model ve vocabulary dosyalarÄ±nÄ±n bulunduÄŸu dizin
            seq_length: Girdi sequence uzunluÄŸu
        """
        self.artifacts_dir = artifacts_dir
        self.seq_length = seq_length
        
        self.model: Model = None
        self.char_to_idx: Dict[str, int] = {}
        self.idx_to_char: Dict[int, str] = {}
        self.vocab_size: int = 0
    
    def load_model_and_vocab(self) -> None:
        """Model ve vocabulary dosyalarÄ±nÄ± yÃ¼kler"""
        try:
            # Model yÃ¼kleme
            model_path = self.artifacts_dir / "best_model.keras"
            if not model_path.exists():
                raise FileNotFoundError(f"Model dosyasÄ± bulunamadÄ±: {model_path}")
            
            logger.info("Model yÃ¼kleniyor...")
            self.model = load_model(model_path)
            logger.info("Model baÅŸarÄ±yla yÃ¼klendi")
            
            # Vocabulary yÃ¼kleme
            char_to_idx_path = self.artifacts_dir / "char_to_idx.json"
            idx_to_char_path = self.artifacts_dir / "idx_to_char.json"
            
            if not char_to_idx_path.exists() or not idx_to_char_path.exists():
                raise FileNotFoundError("Vocabulary dosyalarÄ± bulunamadÄ±")
            
            with open(char_to_idx_path, "r", encoding="utf-8") as f:
                self.char_to_idx = json.load(f)
            
            with open(idx_to_char_path, "r", encoding="utf-8") as f:
                idx_to_char_raw = json.load(f)
                self.idx_to_char = {int(k): v for k, v in idx_to_char_raw.items()}
            
            self.vocab_size = len(self.char_to_idx)
            logger.info(f"Vocabulary yÃ¼klendi (boyut: {self.vocab_size})")
            
        except Exception as e:
            logger.error(f"Model yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def sample_with_temperature(self, predictions: np.ndarray, temperature: float = 1.0) -> int:
        """
        Temperature-based sampling ile sonraki karakteri seÃ§er
        
        Args:
            predictions: Model Ã§Ä±ktÄ±sÄ± (probability distribution)
            temperature: Sampling sÄ±caklÄ±ÄŸÄ±
                - DÃ¼ÅŸÃ¼k (0.2-0.5): Daha deterministik, gÃ¼venli Ã§Ä±ktÄ±
                - Orta (0.5-1.0): Dengeli
                - YÃ¼ksek (1.0+): Daha yaratÄ±cÄ±, riskli
        
        Returns:
            SeÃ§ilen karakterin index'i
        """
        predictions = np.asarray(predictions).astype("float64")
        predictions = np.log(predictions + 1e-8) / temperature
        exp_predictions = np.exp(predictions)
        predictions = exp_predictions / np.sum(exp_predictions)
        
        return np.random.choice(len(predictions), p=predictions)
    
    def generate_text(
        self,
        seed_text: str,
        length: int = 400,
        temperature: float = 0.5
    ) -> str:
        """
        Verilen seed text'ten baÅŸlayarak metin Ã¼retir
        
        Args:
            seed_text: BaÅŸlangÄ±Ã§ metni
            length: Ãœretilecek karakter sayÄ±sÄ±
            temperature: Sampling sÄ±caklÄ±ÄŸÄ±
        
        Returns:
            Ãœretilen metin (seed text dahil)
        """
        if self.model is None:
            raise RuntimeError("Model yÃ¼klenmemiÅŸ!")
        
        generated = seed_text.lower()
        
        for _ in range(length):
            # Son SEQ_LENGTH karakteri al
            seq = generated[-self.seq_length:]
            
            # Padding (seed kÄ±sa ise)
            if len(seq) < self.seq_length:
                seq = " " * (self.seq_length - len(seq)) + seq
            
            # Karakterleri index'lere Ã§evir
            x = np.zeros((1, self.seq_length), dtype=np.int32)
            for t, char in enumerate(seq):
                x[0, t] = self.char_to_idx.get(char, 0)
            
            # Tahmin yap
            predictions = self.model.predict(x, verbose=0)[0]
            next_idx = self.sample_with_temperature(predictions, temperature)
            next_char = self.idx_to_char[next_idx]
            
            generated += next_char
        
        logger.info("Metin Ã¼retimi tamamlandÄ±")
        return generated


# =========================
# CACHED RESOURCES
# =========================
@st.cache_resource
def load_generator() -> StreamlitTextGenerator:
    """
    Text generator'Ä± yÃ¼kler ve cache'ler
    
    Returns:
        YÃ¼klenmiÅŸ StreamlitTextGenerator instance
    """
    generator = StreamlitTextGenerator(
        artifacts_dir=Config.ARTIFACTS_DIR,
        seq_length=Config.SEQ_LENGTH
    )
    generator.load_model_and_vocab()
    return generator

# =========================
# STREAMLIT UI
# =========================
def main():
    """Ana Streamlit uygulamasÄ±"""
    
    # Sayfa yapÄ±landÄ±rmasÄ±
    st.set_page_config(
        page_title="Tolstoy Style Text Generator",
        page_icon="ğŸ­",
        layout="centered",
        initial_sidebar_state="expanded"
    )
    
    # BaÅŸlÄ±k ve aÃ§Ä±klama
    st.title("ğŸ­ Tolstoy Style Text Generator")
    st.markdown(
        """
        Bu uygulama, **LSTM tabanlÄ± character-level** bir derin Ã¶ÄŸrenme modeli kullanarak 
        **Lev Tolstoy stilinde metin Ã¼retimi** yapmaktadÄ±r.
        
        Model, *Anna Karenina* ve *War and Peace* eserleri Ã¼zerinde eÄŸitilmiÅŸtir.
        """
    )
    
    # Model yÃ¼kleme
    try:
        generator = load_generator()
        
        # Model bilgileri
        with st.expander("â„¹ï¸ Model Bilgileri"):
            st.write(f"**Vocabulary Boyutu:** {generator.vocab_size} karakter")
            st.write(f"**Sequence Length:** {generator.seq_length}")
            st.write(f"**Model Tipi:** Bidirectional LSTM")
            st.write(f"**Artifact Dizini:** `{generator.artifacts_dir}`")
        
    except Exception as e:
        st.error(f"âŒ Model yÃ¼klenirken hata oluÅŸtu: {e}")
        st.info("LÃ¼tfen `artifacts/` dizininde gerekli dosyalarÄ±n olduÄŸundan emin olun.")
        logger.error(f"Model yÃ¼kleme hatasÄ±: {e}")
        return
    
    # Sidebar - Kontrol paneli
    st.sidebar.header("âš™ï¸ Ayarlar")
    
    st.sidebar.markdown("### ğŸ“ BaÅŸlangÄ±Ã§ Metni")
    seed_text = st.sidebar.text_area(
        "Seed Text",
        value=Config.DEFAULT_SEED,
        height=100,
        help="Model bu metinden devam ederek yeni metin Ã¼retecek"
    )
    
    st.sidebar.markdown("### ğŸŒ¡ï¸ Temperature")
    temperature = st.sidebar.slider(
        "YaratÄ±cÄ±lÄ±k Seviyesi",
        min_value=Config.MIN_TEMPERATURE,
        max_value=Config.MAX_TEMPERATURE,
        value=Config.DEFAULT_TEMPERATURE,
        step=0.1,
        help="DÃ¼ÅŸÃ¼k: daha tutarlÄ±, YÃ¼ksek: daha yaratÄ±cÄ±"
    )
    
    # Temperature aÃ§Ä±klamasÄ±
    if temperature < 0.5:
        temp_desc = "ğŸ”¹ **DÃ¼ÅŸÃ¼k** - Daha deterministik ve gÃ¼venli Ã§Ä±ktÄ±"
    elif temperature < 0.9:
        temp_desc = "ğŸ”¸ **Orta** - Dengeli yaratÄ±cÄ±lÄ±k"
    else:
        temp_desc = "ğŸ”¶ **YÃ¼ksek** - Daha yaratÄ±cÄ± ve riskli Ã§Ä±ktÄ±"
    st.sidebar.caption(temp_desc)
    
    st.sidebar.markdown("### ğŸ“ Ãœretim UzunluÄŸu")
    length = st.sidebar.slider(
        "Karakter SayÄ±sÄ±",
        min_value=Config.MIN_LENGTH,
        max_value=Config.MAX_LENGTH,
        value=Config.DEFAULT_LENGTH,
        step=50,
        help="Ãœretilecek toplam karakter sayÄ±sÄ±"
    )
    
    # Ãœretim butonu
    st.sidebar.markdown("---")
    generate_btn = st.sidebar.button(
        "âœï¸ Metin Ãœret",
        type="primary",
        use_container_width=True
    )
    
    # Metin Ã¼retimi
    if generate_btn:
        if not seed_text.strip():
            st.warning("âš ï¸ LÃ¼tfen bir seed text girin")
            return
        
        try:
            with st.spinner("ğŸ”„ Metin Ã¼retiliyor... LÃ¼tfen bekleyin."):
                output = generator.generate_text(
                    seed_text=seed_text,
                    length=length,
                    temperature=temperature
                )
            
            # SonuÃ§larÄ± gÃ¶ster
            st.success("âœ… Metin baÅŸarÄ±yla Ã¼retildi!")
            
            # Metin alanÄ±
            st.subheader("ğŸ“œ Ãœretilen Metin")
            st.text_area(
                label="",
                value=output,
                height=400,
                label_visibility="collapsed"
            )
            
            # Ä°statistikler
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Toplam Karakter", len(output))
            with col2:
                st.metric("Kelime SayÄ±sÄ±", len(output.split()))
            with col3:
                st.metric("SatÄ±r SayÄ±sÄ±", output.count('\n') + 1)
            
        except Exception as e:
            st.error(f"âŒ Metin Ã¼retilirken hata oluÅŸtu: {e}")
            logger.error(f"Ãœretim hatasÄ±: {e}")
    
    # Alt bilgi
    st.markdown("---")
    st.caption("ğŸ’¡ **Character-level LSTM** | Generative AI Demo | Tolstoy Corpus")
    
    # Sidebar alt bilgi
    st.sidebar.markdown("---")
    st.sidebar.markdown(
        """
        <div style='text-align: center;'>
        <small>
        ğŸ“ <b>Deep Learning Project</b><br>
        Character-Level Text Generation<br>
        LSTM Neural Network
        </small>
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
